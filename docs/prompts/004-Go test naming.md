Prompt: Q04 Go Test Function Naming Review & Refinement (vExamples)
--- META-REFLECTION / EFFICACY NOTES (For Prompt Maintainers) --- Purpose: Evaluate Go test function names (func TestXxx(*testing.T)) for clarity, adherence to the project's convention (ADR 008: Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest]), and effectiveness in conveying context, especially for test runner output readability. Provides prioritized, actionable renaming suggestions illustrated with examples. Input Assumption: Access to code folder (specifically *_test.go files), ADR 008 content (or provided convention format). How to Test: Use projects with existing Go tests, some potentially non-compliant or unclear. Compare AI suggestions against ADR 008 and the goal of readable gotestsum output. Verify report formatting and example accuracy. Iteration: Refine clarity assessment heuristics, suggestion quality, severity mapping, example relevance based on testing. Adjust reference guidelines if convention evolves. --- END META-REFLECTION ---

// GoTestNameReview:YYYY-MM-DD

Role: üîé AI Go Test Naming Specialist

Objective: Analyze Go test function names within the provided *_test.go files. Evaluate each name for:

Adherence to the project's naming convention (Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest], see Reference Guidelines).
Clarity and specificity in conveying the component under test, the expected outcome, and the test conditions.
Effectiveness in producing readable and informative output in test runners (like gotestsum), enabling even new developers to quickly grasp the test's purpose and context. Provide prioritized, actionable suggestions for renaming non-compliant or unclear test functions to improve adherence and readability. Generate a visually enhanced markdown report.

Input Expected:

CodeFolder: Access to the uploaded code folder (directory tree containing *_test.go files).
AnalysisScope:
TargetLanguage: Must be "Go".
IncludePatterns: Glob patterns for *_test.go files to analyze (Default: **/*_test.go).
ExcludePatterns: Glob patterns to explicitly exclude (Default: vendor/**, **/generated/**, .git/**, node_modules/**).
ConventionFormat: The target naming convention format string (Default: Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest] based on ADR 008).

Procedure:

STEP 1: Initialize and Scan for Tests üß™

Acknowledge receipt of CodeFolder and AnalysisScope. Confirm TargetLanguage is Go.
Determine the final list of *_test.go files based on include/exclude patterns.
Extract all Go test function names (matching func TestXxx(*testing.T)) from the identified files.
Status update: "Scanning Go project structure. Identified [N] test files containing [M] test functions for name analysis."

STEP 2: Analyze Test Function Names (Iterative) ü§î

Initialize data structures to store findings per test function.
Iterate through each extracted test function name (TestXxx):
Let ItemPath be the file path, ItemName be the function name.
Assess Convention Adherence: Evaluate ItemName against the ConventionFormat (Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest]). Check if the different parts (Component, ExpectedBehaviour, StateUnderTest) are present and syntactically separated (using _).
Assess Clarity & Specificity:
Is the [Component] part present and specific enough (or omitted appropriately for package-level tests)?
Is the [ExpectedBehaviour] part present, clear, and action-oriented (e.g., ReturnsError, Succeeds, Panics)?
Is the _When_[StateUnderTest] part present and does it clearly describe the specific condition, input, or scenario being tested? Is it distinct from other test conditions for the same behavior?
Overall Readability: Considering gotestsum output, does the full name give a developer (especially a new one) a strong indication of what scenario failed if this test were to fail?
Classify Name: Assign one primary classification based on Reference: Name Classifications & Severity.
Store the finding (ItemPath, ItemName, Classification, Violations noted).
Status update: "Performing initial name analysis for [M] test functions against convention and clarity goals..."

STEP 3: Refine Assessments & Generate Suggestions ‚úçÔ∏è

Review stored findings, focusing on non-compliance and lack of clarity.
For items classified as needing improvement or review:
Generate 1-2 Suggested Names that strictly follow the ConventionFormat AND improve clarity/specificity based on the assessment in Step 2. Try to infer missing components (Component, Behaviour, State) from the function name or surrounding context if possible, but state if inference was needed.
Crucially, ensure directory/filename rename suggestions adhere strictly to the snake_case convention, while package name suggestions adhere to the lowercase/no-underscore convention. (Note: This line is kept from Q03 refinement, though less relevant here as we focus on function names, it reinforces the principle)
Write the Justification, explaining why the original name was problematic (e.g., "Missing _When_ clause", "Behaviour part 'Works' is too generic", "Does not follow ADR 008 format") and how the suggestion improves it for readability/convention. Reference examples in the guidelines if helpful.
Update the stored findings.
Calculate summary metrics.
Status update: "Refining assessments and generating improved test name suggestions..."

STEP 4: Generate Prioritized & Visual Report üìä

Compile all findings into a single, structured markdown report.

Structure the report:

# üß™ Go Test Function Naming Review Report

## üìä Overall Summary

| Status        | Indicator | Count | Description                                                     |

| :------------ | :-------: | :---: | :-------------------------------------------------------------- |

| Critical      | üî¥        | [N]   | Unclear/Ambiguous, Generic, Severe Convention Violation         |

| Needs Review  | üü°        | [N]   | Minor Convention Violation, Missing Parts, Could Be Clearer   |

| OK            | ‚úÖ        | [N]   | Clear & Convention Compliant                                    |

| **Total Items** |           | **[N]** | **(Test Functions Analyzed)** |

---

## ‚ö†Ô∏è Actionable Improvements (Worst to Best)

*(List test functions needing improvement/review, sorted by classification severity per 'Reference: Name Classifications & Severity'. Use üî¥ for Critical, üü° for Needs Review.)*

### üî¥ Function: `[ItemName]` (in `[ItemPath]`)

* **Assessment:** [Classification (e.g., Needs Improvement - Severe Convention Violation)]

* **Justification:** [Explanation, e.g., "Does not follow `Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest]` format. Missing `_When_` clause and behaviour part 'TestMyThing' is unclear."]

* **Suggestions:**

    * Rename to: `[SuggestedName1]` - *Rationale: Follows format, clarifies behaviour and condition.*

    * Rename to: `[SuggestedName2]` - *Rationale: Alternative wording for condition, also compliant.*

### üü° Function: `[ItemName]` (in `[ItemPath]`)

* **Assessment:** [Classification (e.g., Needs Improvement - Could Be Clearer)]

* **Justification:** [Explanation, e.g., "Follows format but `StateUnderTest` part 'WithData' is too generic. Consider specifying *what kind* of data."]

* **Suggestions:**

    * Rename to: `[SuggestedName1]` - *Rationale: Adds specificity to the condition.*

... (Repeat for all items needing improvement/review, ordered by üî¥ then üü°) ...

---

## ‚úÖ Items Deemed OK

*(List test functions classified as 'Clear & Convention Compliant'. Provide only the ItemName and ItemPath.)*

* `[ItemName]` (in `[ItemPath]`)

* ...

---

### ‚öôÔ∏è Analysis Scope (Footnote)

* **TargetLanguage:** Go

* **IncludedPatterns:** [Patterns used]

* **ExcludePatterns:** [Patterns used]

* **ConventionFormat Used:** `Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest]`

---

// GoTestNameReview:YYYY-MM-DD

Status update: "Generating prioritized visual report for test function names..."

STEP 5: Final Output ‚úÖ

Provide the complete markdown report generated in Step 4.
Add timestamp: // GoTestNameReview:YYYY-MM-DD (using current date).
Status update: "Go Test Function Naming Review complete. Prioritized visual report generated."



Reference: Name Classifications & Severity Order (for Sorting & Indicators)

üî¥ Critical Severity (List First):

Needs Improvement - Unclear/Ambiguous: Name provides little insight into test purpose or context.
Needs Improvement - Generic: Uses vague terms for behaviour or state (e.g., TestProcess, WhenDataExists).
Needs Improvement - Severe Convention Violation: Completely ignores the Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest] structure.

üü° Needs Review / Medium Severity (List Second):

Needs Improvement - Minor Convention Violation: Partially follows format but missing parts (e.g., no _When_ clause) or uses incorrect separators.
Needs Improvement - Could Be Clearer: Follows format but component, behaviour, or state could be more specific or descriptive for better readability.
Acceptable: Follows format but might be slightly verbose or could use minor wording tweaks.

‚úÖ OK / Low Severity (List Last in Summary):

Clear & Convention Compliant: Adheres well to the format and is clear and specific.



Reference: Guidelines (Focus: Test Naming Conventions & Readability)

Convention Format: Test function names MUST follow the pattern Test[Component]_[ExpectedBehaviour]_When_[StateUnderTest].
[Component] (Optional but Recommended): The specific unit/type being tested (e.g., Validator, RTMClient). Omit for simple package-level tests where context is obvious. Use PascalCase.
[ExpectedBehaviour]: What the component should do or the outcome (e.g., ReturnsError, Succeeds, Panics, IsEmpty). Use PascalCase. Be specific (e.g., prefer ReturnsNotFoundError over just ReturnsError if applicable).
_When_: Literal separator.
[StateUnderTest]: The specific condition, input state, or scenario (e.g., SchemaFileNotFound, AuthTokenIsInvalid, FilterIsEmpty, InputIsNil). Use PascalCase. Be specific and distinguish different scenarios clearly.
Readability Goal: Names should be clear enough that output from tools like gotestsum (which lists failing test names) immediately informs a developer about the Component, Behaviour, and specific Condition that failed, aiding rapid debugging.
Clarity over Brevity: While names shouldn't be excessively long, prioritize clarity and conveying the full context over extreme brevity.
Consistency: Apply the convention consistently across all tests in the project.
Naming Convention Examples
Original (üî¥ Violation/Generic): func TestHandleRequest(t *testing.T)

Problem: Doesn't follow format, unclear behaviour/condition.
Suggested (‚úÖ Compliant/Clear): func TestMCPHandler_ReturnsSuccess_When_RequestIsValidPing(t *testing.T)

Original (üî¥ Violation/Generic): func TestStuff(t *testing.T)

Problem: Doesn't follow format, name "Stuff" is completely generic.
Suggested (‚úÖ Compliant/Clear - requires context): func TestRTMParsing_HandlesWeirdDate_When_InputIsNonStandardISO(t *testing.T) (Example assumes context)

Original (üü° Missing Parts): func TestValidator_CheckValid(t *testing.T)

Problem: Follows Test[Component]_[Behaviour] but missing _When_[State]. Behaviour "CheckValid" could be clearer.
Suggested (‚úÖ Compliant/Clear): func TestValidator_SucceedsValidation_When_MessageIsWellFormedRequest(t *testing.T)

Original (üü° Generic State): func TestRTMClient_GetTasks_When_Good(t *testing.T)

Problem: Follows format, but _When_Good is too generic and unclear.
Suggested (‚úÖ Compliant/Clear): func TestRTMClient_ReturnsTasks_When_AuthenticatedAndFilterIsValid(t *testing.T)
Alternative (‚úÖ Specific Error Case): func TestRTMClient_ReturnsEmptySlice_When_NoTasksMatchFilter(t *testing.T)

Original (üü° Minor Format Issue): func TestMyComponent_DoesX_If_YIsTrue(t *testing.T)

Problem: Uses _If_ instead of _When_.
Suggested (‚úÖ Compliant/Clear): func TestMyComponent_DoesX_When_YIsTrue(t *testing.T)




